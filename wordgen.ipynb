{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20430d34",
   "metadata": {},
   "source": [
    "•\tRandom Word Generation: Selecting words randomly from a predefined word corpus (dictionary) like the NLTK words dataset. \n",
    "•\tWord Validity Check: Verifying if a word exists in a dictionary corpus. \n",
    "•\tGenerating Words from Characters: Creating possible words by permuting given characters and filtering valid ones. \n",
    "•\tAdding Prefixes and Suffixes: Modifying words by attaching common prefixes (e.g., \"un-\", \"pre-\") or suffixes (e.g., \"-ing\", \"-ness\") and checking if they form real words. \n",
    "•\tExtracting Unique Words & Computing Word Length: Filtering words by removing stopwords (common words like \"the\", \"is\") and punctuation, then measuring word length statistics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ba49733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "743924df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\divya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('words')\n",
    "word_list= nltk.corpus.words.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "766b2e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['supercabinet', 'refectorial', 'supererogator', 'omninescience', 'Faliscan', 'supramental', 'craggy', 'noncontributor', 'radiescent', 'possessed']\n"
     ]
    }
   ],
   "source": [
    "n = int(input(\"Enter the number of random words you want: \"))\n",
    "random_word= random.sample(word_list,n)\n",
    "print(random_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8cca783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid word\n"
     ]
    }
   ],
   "source": [
    "word= input(\"Enter a word: \")\n",
    "if word in word_list:\n",
    "    print(\"valid word\")\n",
    "else:\n",
    "    print(\"invalid word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "494fea99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'act', 'sec', 'ear', 'ras', 'eta', 'car', 'are', 'rea', 'tar', 'tea', 'arc', 'tra', 'ace', 'tec', 'cat', 'art', 'era', 'set', 'tst', 'sea', 'sac', 'sar', 'ers', 'ate', 'ser', 'tae', 'sat', 'ast', 'aes', 'ase', 'aer', 'rat', 'eat', 'tat', 'ret'}\n"
     ]
    }
   ],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "def generatewords(word,length):\n",
    "    return [''.join(p)for p in permutations(word,length) if ''.join(p).lower() in set(word_list) ]\n",
    "word= input(\"Enter a word: \")\n",
    "length= int(input(\"Enter the length of the word: \"))\n",
    "words= generatewords(word,length)\n",
    "print(set(words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfb59abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid words with prefixes/suffixes:  {'comforter', 'recomfort', 'discomfort', 'comforting', 'uncomfort'}\n"
     ]
    }
   ],
   "source": [
    "# Define some common prefixes and suffixes (can be extended)\n",
    "prefixes = ['un', 're', 'dis', 'in', 'im']\n",
    "suffixes = ['ed', 'ing', 'es', 'er', 'ly']\n",
    "word= input(\"Enter a word: \")\n",
    "valid= set()\n",
    "\n",
    "for prefix in prefixes:\n",
    "    new_word= prefix+ word\n",
    "    if new_word in word_list:\n",
    "        valid.add(new_word)\n",
    "for suffix in suffixes:\n",
    "    new_word= word+ suffix\n",
    "    if new_word in word_list:\n",
    "        valid.add(new_word)\n",
    "print(\"Valid words with prefixes/suffixes: \", valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "088b2b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\divya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\divya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words: 49621\n",
      "Average word length: 8.09\n",
      "Minimum word length: 1\n",
      "Maximum word length: 33\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import brown, stopwords\n",
    "import string\n",
    "\n",
    "# Download required NLTK datasets\n",
    "nltk.download('brown')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define stopwords and punctuation\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "# Extract words from the Brown corpus\n",
    "words = brown.words()\n",
    "\n",
    "# Filter words: Remove stopwords and punctuation\n",
    "filtered_words = [word.lower() for word in words if word.lower() not in stop_words and word not in punctuation]\n",
    "\n",
    "# Extract unique words\n",
    "unique_words = set(filtered_words)\n",
    "\n",
    "# Compute word length statistics\n",
    "word_lengths = [len(word) for word in unique_words]\n",
    "average_length = sum(word_lengths) / len(word_lengths) if word_lengths else 0\n",
    "min_length = min(word_lengths) if word_lengths else 0\n",
    "max_length = max(word_lengths) if word_lengths else 0\n",
    "\n",
    "# Output the statistics\n",
    "print(f\"Total unique words: {len(unique_words)}\")\n",
    "print(f\"Average word length: {average_length:.2f}\")\n",
    "print(f\"Minimum word length: {min_length}\")\n",
    "print(f\"Maximum word length: {max_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa78f79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\divya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\divya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: in the\n",
      "\n",
      "Bigram Predictions:\n",
      "Brown: [('first', 0.009461062440153777), ('same', 0.008975146846550713), ('most', 0.005959611839190522)]\n",
      "WSJ  : [('company', 0.0327455919395466), ('u.s.', 0.02392947103274559), ('new', 0.014693534844668345)]\n",
      "\n",
      "Trigram Predictions:\n",
      "Brown: [('world', 0.014937759336099586), ('first', 0.014605809128630706), ('united', 0.012282157676348548)]\n",
      "WSJ  : [('u.s.', 0.07435897435897436), ('past', 0.041025641025641026), ('first', 0.02564102564102564)]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown, treebank\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('treebank')\n",
    "\n",
    "def prepare(corpus):\n",
    "    return [[w.lower() for w in s] for s in corpus]\n",
    "\n",
    "def build_ngram_model(sentences, n):\n",
    "    ngram_counts, context_counts = Counter(), Counter()\n",
    "    for sent in sentences:\n",
    "        sent = ['<s>'] * (n - 1) + sent + ['</s>']\n",
    "        for i in range(len(sent) - n + 1):\n",
    "            ngram = tuple(sent[i:i + n])\n",
    "            context = ngram[:-1]\n",
    "            ngram_counts[ngram] += 1\n",
    "            context_counts[context] += 1\n",
    "    return ngram_counts, context_counts\n",
    "\n",
    "def predict_next(ngram_counts, context_counts, context):\n",
    "    context = tuple(context)\n",
    "    candidates = [(ng[-1], count / context_counts[context])\n",
    "                  for ng, count in ngram_counts.items() if ng[:-1] == context]\n",
    "    return sorted(candidates, key=lambda x: -x[1])[:3]\n",
    "\n",
    "# Load and preprocess\n",
    "brown_sents = prepare(brown.sents())\n",
    "wsj_sents = prepare(treebank.sents())\n",
    "\n",
    "# Build models\n",
    "big_brown, ctx_big_brown = build_ngram_model(brown_sents, 2)\n",
    "tri_brown, ctx_tri_brown = build_ngram_model(brown_sents, 3)\n",
    "big_wsj, ctx_big_wsj = build_ngram_model(wsj_sents, 2)\n",
    "tri_wsj, ctx_tri_wsj = build_ngram_model(wsj_sents, 3)\n",
    "\n",
    "# Predict\n",
    "def run_predict(text):\n",
    "    words = text.lower().split()\n",
    "    print(f\"Input: {text}\\n\")\n",
    "    \n",
    "    if len(words) >= 1:\n",
    "        ctx = (words[-1],)\n",
    "        print(\"Bigram Predictions:\")\n",
    "        print(\"Brown:\", predict_next(big_brown, ctx_big_brown, ctx))\n",
    "        print(\"WSJ  :\", predict_next(big_wsj, ctx_big_wsj, ctx))\n",
    "        \n",
    "    if len(words) >= 2:\n",
    "        ctx = (words[-2], words[-1])\n",
    "        print(\"\\nTrigram Predictions:\")\n",
    "        print(\"Brown:\", predict_next(tri_brown, ctx_tri_brown, ctx))\n",
    "        print(\"WSJ  :\", predict_next(tri_wsj, ctx_tri_wsj, ctx))\n",
    "\n",
    "# Example usage\n",
    "run_predict(\"in the\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f317109b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\divya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\divya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: in the\n",
      "\n",
      "Bigram Predictions:\n",
      "Brown: [('first', 0.009461062440153777), ('same', 0.008975146846550713), ('most', 0.005959611839190522)]\n",
      "Top 10 Brown sentences with 'in the first':\n",
      "1. <s> meanwhile , it was learned the state highway department is very near being ready to issue the first $30 million worth of highway reconstruction bonds . </s>\n",
      "2. <s> operating budget for the day schools in the five counties of dallas , harris , bexar , tarrant and el paso would be $451,500 , which would be a savings of $157,460 yearly after the first year's capital outlay of $88,000 was absorbed , parkhouse told the senate . </s>\n",
      "3. <s> the president spent much of the week-end at his summer home on cape cod writing the first drafts of portions of the address with the help of white house aids in washington with whom he talked by telephone . </s>\n",
      "4. <s> the social security payroll tax is now 6 per cent -- 3 per cent on each worker and employer -- on the first $4,800 of pay per year . </s>\n",
      "5. <s> officials estimated the annual tax boost for the medical plan would amount to 1.5 billion dollars and that medical benefits paid out would run 1 billion or more in the first year , 1963 . </s>\n",
      "6. <s> full payment of hospital bills for stays up to 90 days for each illness , except that the patient would pay $10 a day of the cost for the first nine days . </s>\n",
      "7. <s> the scholarship plan would provide federal contributions to each medical and dental school equal to $1,500 a year for one-fourth of the first year students . </s>\n",
      "8. <s> officials estimated the combined programs would cost 5.1 million dollars the first year and would go up to 21 millions by 1966 . </s>\n",
      "9. <s> the kennedy administration moves haltingly toward a geneva conference on laos just as serious debate over its foreign policy erupts for the first time . </s>\n",
      "10. <s> one of the first things he would do , he said , would be to organize classes in first aid . </s>\n",
      "WSJ: [('company', 0.0327455919395466), ('u.s.', 0.02392947103274559), ('new', 0.014693534844668345)]\n",
      "Top 10 WSJ sentences with 'in the company':\n",
      "1. <s> from 1953 to 1955 , 9.8 billion kent cigarettes with the filters were sold *-3 , the company said 0 *t*-1 . </s>\n",
      "2. <s> john rowe , president *rnr*-1 and chief executive officer *rnr*-1 of new england electric , said 0 the company 's return on equity could suffer if it made a higher bid and its forecasts related * to ps of new hampshire -- such as growth in electricity demand and improved operating efficiencies -- did n't come true . </s>\n",
      "3. <s> it was just another one of the risk factors '' that *t*-17 led to the company 's decision * to withdraw from the bidding , he added 0 *t*-1 . </s>\n",
      "4. <s> the new plant , located * in chinchon about 60 miles from seoul , will help *-2 meet increasing and diversifying demand for control products in south korea , the company said 0 *t*-1 . </s>\n",
      "5. <s> moreover , there have been no orders for the cray-3 so far , though the company says 0 it is talking with several prospects . </s>\n",
      "6. <s> without the cray-3 research and development expenses , the company would have been able *-2 to report a profit of $ 19.3 million *u* *ich*-3 for the first half of 1989 rather than the $ 5.9 million *u* 0 it posted *t*-1 . </s>\n",
      "7. <s> besides messrs. cray and barnum , other senior management at the company includes neil davenport , 47 , president and chief executive officer ; joseph m. blanchard , 37 , vice president , engineering ; malcolm a. hammerton , 40 , vice president , software ; and douglas r. wheeland , 45 , vice president , hardware . </s>\n",
      "8. <s> arthur a. hatch , 59 , was named *-28 executive vice president of the company . </s>\n",
      "9. <s> he was previously president of the company 's eastern edison co. unit . </s>\n",
      "10. <s> magna international inc. 's chief financial officer , james mcalpine , resigned and its chairman , frank stronach , is stepping in *-1 to help turn the automotive-parts manufacturer around , the company said 0 *t*-2 . </s>\n",
      "\n",
      "Trigram Predictions:\n",
      "Brown: [('world', 0.014937759336099586), ('first', 0.014605809128630706), ('united', 0.012282157676348548)]\n",
      "Top 10 Brown sentences with 'in the world':\n",
      "1. <s> <s> `` i've become the greatest beggar in the world '' . </s>\n",
      "2. <s> <s> `` all we have left in the world is one another , and we must stay together the way mother wanted '' , kowalski said in telling his children of their mother's death yesterday afternoon . </s>\n",
      "3. <s> <s> i wasn't sure there were such people anywhere in the world . </s>\n",
      "4. <s> <s> displaying his knowledge of music , the new england-born president remarked that `` probably the best chamber music in the world is played in vermont , by young americans -- and here in this school where they have produced extraordinary musicians and teachers , and their work is being duplicated all across the united states . </s>\n",
      "5. <s> <s> political , economic and military experts were all agreed that chaotic , mountainous little laos was the last place in the world to fight a war -- and they were probably right . </s>\n",
      "6. <s> <s> he gathered one of the biggest collections of paul klees in the world . </s>\n",
      "7. <s> <s> among the highest-paid workers in the world are u.s. coal miners . </s>\n",
      "8. <s> <s> a fundamental source of knowledge in the world today is the book found in our libraries . </s>\n",
      "9. <s> <s> a driver of a dairy truck , who begins work at 1 a.m. finishes before breakfast , then goes out and grows a garden , and who has used the cannery to save and feed a family of five , asked , `` what in the world will we do '' ? ? </s>\n",
      "10. <s> <s> `` what in the world '' , echoed others , those come with the beans , potatoes , the tomatoes , `` will any of us do '' ? ? </s>\n",
      "WSJ: [('u.s.', 0.07435897435897436), ('past', 0.041025641025641026), ('first', 0.02564102564102564)]\n",
      "Top 10 WSJ sentences with 'in the u.s.':\n",
      "1. <s> <s> the white house said 0 president bush has approved duty-free treatment for imports of certain types of watches that *t*-35 are n't produced *-1 in `` significant quantities '' in the u.s. , the virgin islands and other u.s. possessions . </s>\n",
      "2. <s> <s> the action came in response to a petition filed * by timex inc. for changes in the u.s. generalized system of preferences for imports from developing nations . </s>\n",
      "3. <s> <s> the white house said 0 mr. bush decided *-1 to grant duty-free status for 18 categories , but turned down such treatment for other types of watches `` because of the potential for material injury to watch producers located * in the u.s. and the virgin islands . '' </s>\n",
      "4. <s> <s> they argue that u.s. investors often can buy american depositary receipts on the big stocks in many funds ; these so-called adrs represent shares of foreign companies traded * in the u.s. . </s>\n",
      "5. <s> <s> if the debts are repaid *-49 , it could clear the way 0 for soviet bonds to be sold *-50 in the u.s. *t*-1 . </s>\n",
      "6. <s> <s> but the soviets might still face legal obstacles to * raising money in the u.s. until they settle hundreds of millions of dollars in additional debt still outstanding from the world war ii lend-lease program . </s>\n",
      "7. <s> <s> interviews with analysts and business people in the u.s. suggest that japanese capital may produce the economic cooperation that southeast asian politicians have pursued *t*-2 in fits and starts for decades . </s>\n",
      "8. <s> <s> they point out that these institutions want *-1 to lock in returns on high-yield u.s. treasury debt and suggest 0 demand for the u.s. unit will continue *-2 unabated until rates in the u.s. recede . </s>\n",
      "9. <s> <s> safety advocates , including some members of congress , have been urging the department for years *-2 to extend car-safety requirements to light trucks and vans , which now *t*-157 account for almost one-third of all vehicle sales in the u.s. . </s>\n",
      "10. <s> <s> in every major market in the u.s. , for instance , you can buy '86 la tache or richebourg , virtually all of the first growth bordeaux -lrb- except petrus -rrb- , as well as opus one and dominus from california and , at the moment , the stag 's leap 1985 cask 23 . </s>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown, treebank\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('treebank')\n",
    "\n",
    "def build_model(corpus, n):\n",
    "    model, context = Counter(), Counter()\n",
    "    prepped = []\n",
    "    for sent in corpus:\n",
    "        sent = ['<s>']*(n-1) + [w.lower() for w in sent] + ['</s>']\n",
    "        prepped.append(sent)\n",
    "        for i in range(len(sent)-n+1):\n",
    "            ng = tuple(sent[i:i+n])\n",
    "            model[ng] += 1\n",
    "            context[ng[:-1]] += 1\n",
    "    return model, context, prepped\n",
    "\n",
    "def predict(model, context, ctx):\n",
    "    ctx = tuple(ctx)\n",
    "    candidates = [(w[-1], c / context[ctx]) for w, c in model.items() if w[:-1] == ctx]\n",
    "    return sorted(candidates, key=lambda x: -x[1])[:3]\n",
    "\n",
    "def find_sentences(corpus, phrase, next_word):\n",
    "    out = []\n",
    "    for sent in corpus:\n",
    "        for i in range(len(sent) - len(phrase)):\n",
    "            if sent[i:i+len(phrase)] == phrase and i + len(phrase) < len(sent):\n",
    "                if sent[i + len(phrase)] == next_word:\n",
    "                    out.append(' '.join(sent))\n",
    "                    break\n",
    "        if len(out) == 10: break\n",
    "    return out\n",
    "\n",
    "# Build models\n",
    "b2, cb2, brown2 = build_model(brown.sents(), 2)\n",
    "b3, cb3, brown3 = build_model(brown.sents(), 3)\n",
    "w2, cw2, wsj2 = build_model(treebank.sents(), 2)\n",
    "w3, cw3, wsj3 = build_model(treebank.sents(), 3)\n",
    "\n",
    "def run(text):\n",
    "    words = text.lower().split()\n",
    "    print(f\"\\nInput: {text}\")\n",
    "    \n",
    "    if len(words) >= 1:\n",
    "        ctx = [words[-1]]\n",
    "        print(\"\\nBigram Predictions:\")\n",
    "        for label, model, ctxs, corpus in [(\"Brown\", b2, cb2, brown2), (\"WSJ\", w2, cw2, wsj2)]:\n",
    "            pred = predict(model, ctxs, ctx)\n",
    "            print(f\"{label}:\", pred)\n",
    "            if pred:\n",
    "                top = pred[0][0]\n",
    "                matches = find_sentences(corpus, ctx, top)\n",
    "                print(f\"Top 10 {label} sentences with '{text} {top}':\")\n",
    "                for i, s in enumerate(matches, 1): print(f\"{i}. {s}\")\n",
    "                if not matches: print(\"No matching sentences found.\")\n",
    "\n",
    "    if len(words) >= 2:\n",
    "        ctx = words[-2:]\n",
    "        print(\"\\nTrigram Predictions:\")\n",
    "        for label, model, ctxs, corpus in [(\"Brown\", b3, cb3, brown3), (\"WSJ\", w3, cw3, wsj3)]:\n",
    "            pred = predict(model, ctxs, ctx)\n",
    "            print(f\"{label}:\", pred)\n",
    "            if pred:\n",
    "                top = pred[0][0]\n",
    "                matches = find_sentences(corpus, ctx, top)\n",
    "                print(f\"Top 10 {label} sentences with '{text} {top}':\")\n",
    "                for i, s in enumerate(matches, 1): print(f\"{i}. {s}\")\n",
    "                if not matches: print(\"No matching sentences found.\")\n",
    "\n",
    "# Example usage\n",
    "run(\"in the\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a88350d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\divya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\divya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m         probs[k] \u001b[38;5;241m=\u001b[39m {k2: c \u001b[38;5;241m/\u001b[39m total \u001b[38;5;28;01mfor\u001b[39;00m k2, c \u001b[38;5;129;01min\u001b[39;00m v\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, defaultdict) \u001b[38;5;28;01melse\u001b[39;00m v \u001b[38;5;241m/\u001b[39m total\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m probs\n\u001b[1;32m---> 33\u001b[0m emission_probs \u001b[38;5;241m=\u001b[39m normalize(emission_counts)\n\u001b[0;32m     34\u001b[0m transition_probs \u001b[38;5;241m=\u001b[39m normalize(transition_counts, tag_counts)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# POS tagging for user input sentence\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 29\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(counts, total_counts)\u001b[0m\n\u001b[0;32m     27\u001b[0m probs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m counts\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m---> 29\u001b[0m     total \u001b[38;5;241m=\u001b[39m total_counts\u001b[38;5;241m.\u001b[39mget(k, \u001b[38;5;28msum\u001b[39m(v\u001b[38;5;241m.\u001b[39mvalues())) \u001b[38;5;28;01mif\u001b[39;00m total_counts \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m     30\u001b[0m     probs[k] \u001b[38;5;241m=\u001b[39m {k2: c \u001b[38;5;241m/\u001b[39m total \u001b[38;5;28;01mfor\u001b[39;00m k2, c \u001b[38;5;129;01min\u001b[39;00m v\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, defaultdict) \u001b[38;5;28;01melse\u001b[39;00m v \u001b[38;5;241m/\u001b[39m total\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m probs\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from collections import defaultdict\n",
    "\n",
    "# Download necessary data\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "# Load tagged sentences and initialize counts\n",
    "tagged_sentences = brown.tagged_sents(tagset='universal')\n",
    "emission_counts = defaultdict(int)\n",
    "transition_counts = defaultdict(lambda: defaultdict(int))\n",
    "tag_counts = defaultdict(int)\n",
    "\n",
    "# Count emissions and transitions\n",
    "for sentence in tagged_sentences:\n",
    "    prev_tag = None\n",
    "    for word, tag in sentence:\n",
    "        emission_counts[(word.lower(), tag)] += 1\n",
    "        tag_counts[tag] += 1\n",
    "        if prev_tag:\n",
    "            transition_counts[prev_tag][tag] += 1\n",
    "        prev_tag = tag\n",
    "\n",
    "# Normalize counts to probabilities\n",
    "def normalize(counts, total_counts=None):\n",
    "    probs = {}\n",
    "    for k, v in counts.items():\n",
    "        total = total_counts.get(k, sum(v.values())) if total_counts else sum(v.values())\n",
    "        probs[k] = {k2: c / total for k2, c in v.items()} if isinstance(v, defaultdict) else v / total\n",
    "    return probs\n",
    "\n",
    "emission_probs = normalize(emission_counts)\n",
    "transition_probs = normalize(transition_counts, tag_counts)\n",
    "\n",
    "# POS tagging for user input sentence\n",
    "def tag_sentence(sentence):\n",
    "    words = sentence.split()\n",
    "    tags, prev_tag = [], 'NOUN'\n",
    "    for word in words:\n",
    "        word_lower = word.lower()\n",
    "        emission_prob = emission_probs.get((word_lower, 'NOUN'), 1e-6)\n",
    "        transition_prob = transition_probs.get(prev_tag, {}).get('NOUN', 1e-6)\n",
    "        best_tag = max(emission_probs.get(word_lower, {}), key=lambda tag: emission_prob * transition_prob)\n",
    "        tags.append((word, best_tag))\n",
    "        prev_tag = best_tag\n",
    "    return \" \".join([f\"{word}/{tag}\" for word, tag in tags])\n",
    "\n",
    "# Example usage\n",
    "user_input = input(\"Enter a sentence: \")\n",
    "print(tag_sentence(user_input))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80494ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
